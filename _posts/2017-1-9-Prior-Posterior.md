---
layout: post
title: Prior and Posterior
---

<p class="meta">7 January 2017 - Beijing</p>

哇哦，新年第一篇啊。这次主要是想总结一下自己对先验概率和和后验概率的认识和思考吧。 虽然之前听到过很多遍，也觉得理解他们不是一件难事，
但是还是想系统地总结一下目前认识。之所以想这么郑重地总结，是因为概率分布实在是机器学习中重要得不能再重要的工具了，良好的，不浮皮潦草
的理解还是很必要的。

*首先还是知人论世的原则，何谓先验概率，何谓后验概率？*

在我们的机器学习的语境中，我们所指的先验是在**没有看到数据之前**我们认为的**模型的参数分布**。在这里有几个重要次词，已经加粗。
每一个机器学习算法都有其假设，这个假设是灵魂。 假设体现在数学或程序中就是模型的参数的分布。一个好的先验不仅要包括物理意义上的合理性
还要具有计算和运算上的方便性。 我们可以把我们的先验用公式表达（比如假设某参数服从正态分布），也可以用某些条件表达（比如卷积神经网络的weight sharing, sparse connection).
在这里我们暂时专注于可用公式表达的先验，这样更具有数学意义上的严谨性，深度学习的先验难以公式化，这在一定程度上造成了深度学习的难解释的尴尬。

而后验概率是指在**观察到数据后**修正后的**模型的参数分布**，要注意这里面有一个对先验概率的修正过程，这个修正任务是要由似然值和归一化normalization
完成，但是normalization难求，所以计算上的方便性有时显得很重要，不过即便难求，我们还是可以利用sampling和approximate inference的手段完成积分项的近似求解。

一般来讲如果我们的模型是二项分布的模型，那么我们习惯于选择Beta分布作为其参数的先验，这个选择的过程首先是基于计算上的方便性的，因为我们知道，二项分布
的似然函数的形态是一堆指数项的乘积，为了得到方便易于表示的后验概率，我们选择了和似然有类似形态的Beta分布作为先验，也就是说这个选择的过程是基于似然函数的形态的。

同理，对于多项分布的模型，我们选择了狄利克雷分布作为参数先验，对于高斯分布的模型我们采用高斯分布作为参数先验。

其实我们想问的一点是为什么后验概率可以降低先验概率对参数分布描述的不确定性。

假设我们要对参数$$\theta$$进行Bayesian推理，同时我们有数据集$$D$$，这个过程被描述成$$p(\theta, D)$$.
我们注意到：

$$ \mathbb{E}_\theta[\theta] = \mathbb{E}_D[\mathbb{E}_\theta[\theta|D]] $$ 

$$\mathbb{E}_\theta[\theta] = \int p(\theta)\theta d\theta$$

$$\mathbb{E}_D[\mathbb{E}_\theta[\theta|D]] = \int \left \{ \int \theta p(\theta|D) d\theta\right \} p(D) dD$$

这意味着左边的先验概率的均值，和右边后验概率的均值在所有数据分布$$D$$上的期望是相同的。

接下来再看：

$$var_\theta[\theta] = \mathbb{E}_D[var_\theta[\theta|D]] + var_D[\mathbb{E}_\theta[\theta|D]]$$

这个式子就比上面的有意思多了，左边等于是先验概率分布的方差， 右边第一项是后验概率分布的方差在不同的数据分布$$D$$上的期望，第二项是后验概率期望在不同数据分布$$D$$上的方差大小
我们注意到第二项始终不小于零，也就是说总得来讲，后验概率分布的方差要比先验概率的方差小，体现在图像上就是后验概率更加集中，不确定度更加小。这样它的物理意义也被解释出来了。

由此观之，从数学意义上推导，先验概率也要比后验概率更加分散。